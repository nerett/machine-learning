{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./task2.png)\n",
    "\n",
    "- [src](https://habr.com/ru/companies/wunderfund/articles/331310/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVUW3yIlrHu5"
   },
   "source": [
    "# LSTM testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BC3Te-tYrHu8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "P0RVGWO-rHu8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nerus as ns\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "\n",
    "class NerusTokenizer:\n",
    "    words_to_id: dict\n",
    "    ids_to_word: dict\n",
    "    word_next_id: int = 0\n",
    "    tag_to_id: dict\n",
    "    lstm_tokens: list[str] = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[UNK]\"]\n",
    "    def __init__(self):\n",
    "        self.words_to_id = {}\n",
    "        self.tag_to_id = {}\n",
    "        self.ids_to_word = {}\n",
    "        for sys_id, sys_tag in enumerate(self.lstm_tokens):\n",
    "            self.ids_to_word[sys_id] = sys_tag\n",
    "            self.words_to_id[sys_tag] = sys_id\n",
    "            self.tag_to_id[sys_tag] = sys_id\n",
    "        self.word_next_id = self.lstm_tokens.__len__()\n",
    "\n",
    "    def expand_vocabulary(self, sents: list[ns.NerusSent]):\n",
    "        for sent in sents:\n",
    "            for token in sent.tokens:\n",
    "                if token.text in self.words_to_id.keys():\n",
    "                    continue\n",
    "                self.ids_to_word[self.word_next_id] = token.text\n",
    "                self.words_to_id[token.text] = self.word_next_id\n",
    "                self.word_next_id += 1\n",
    "                                                                      # tokenized sentences   # tags strings\n",
    "    def tokenize_sent_pos_tagging(self, sents: list[ns.NerusSent]) -> tuple[list[list[int]], list[list[str]]]:\n",
    "        sentences_ids = []\n",
    "        pos_tags_txts = []\n",
    "        unknown_id = self.words_to_id[\"[UNK]\"]\n",
    "\n",
    "        for sent in sents:\n",
    "            sentence_id = [self.words_to_id[\"[CLS]\"]]\n",
    "            pos_tag_txt = [\"[CLS]\"]\n",
    "            for token in sent.tokens:\n",
    "                word_text = token.text\n",
    "                pos_tag   = token.pos\n",
    "\n",
    "                word_id = self.words_to_id.get(word_text, None)\n",
    "                sentence_id.append(word_id) if word_id is not None else sentence_id.append(unknown_id)\n",
    "                pos_tag_txt.append(pos_tag) if word_id is not None else pos_tag_txt.append(\"[UNK]\")\n",
    "\n",
    "            assert sentence_id.__len__() == pos_tag_txt.__len__()\n",
    "\n",
    "            # separating and padding\n",
    "            pad_id = self.words_to_id[\"[PAD]\"]\n",
    "            sep_id = self.words_to_id[\"[SEP]\"]\n",
    "\n",
    "            if len(sentence_id) < MAX_SENTENCE_LENGTH:\n",
    "                sentence_id.append(sep_id)\n",
    "                sentence_id.extend([pad_id]  * (MAX_SENTENCE_LENGTH - len(sentence_id)))\n",
    "                pos_tag_txt.append(\"[SEP]\")\n",
    "                pos_tag_txt.extend([\"[PAD]\"] * (MAX_SENTENCE_LENGTH - len(pos_tag_txt)))\n",
    "            else:\n",
    "                sentence_id = sentence_id[:MAX_SENTENCE_LENGTH]\n",
    "                sentence_id[-1] = sep_id\n",
    "                pos_tag_txt = pos_tag_txt[:MAX_SENTENCE_LENGTH]\n",
    "                pos_tag_txt[-1] = \"[SEP]\"\n",
    "\n",
    "            assert sentence_id.__len__() == pos_tag_txt.__len__()\n",
    "\n",
    "            sentences_ids.append(sentence_id)\n",
    "            pos_tags_txts.append(pos_tag_txt)\n",
    "\n",
    "        return sentences_ids, pos_tags_txts\n",
    "\n",
    "    def expand_pos_tagging_vocab(self, sents: ns.NerusSent):\n",
    "        new_tags = set()\n",
    "        for sent in sents:\n",
    "            for token in sent.tokens:\n",
    "                new_tags.add(token.pos)\n",
    "        new_tags = [tag for tag in new_tags if tag not in self.tag_to_id.keys()]\n",
    "        for tag in new_tags:\n",
    "            self.tag_to_id[tag] = self.tag_to_id.__len__()\n",
    "\n",
    "    def tokenize_pos_tags(self, text_tags: list[list[str]]) -> list[list[int]]:\n",
    "        tags_id = []\n",
    "        for sent_tags in text_tags:\n",
    "            sent_tags_id = []\n",
    "            for tag in sent_tags:\n",
    "                if tag not in self.tag_to_id.keys():\n",
    "                    msg = f\"Bad tag when put ids on tags; TAG: {tag}\"\n",
    "                    raise RuntimeError(msg)\n",
    "                sent_tags_id.append(self.tag_to_id[tag])\n",
    "            tags_id.append(sent_tags_id)\n",
    "\n",
    "        return tags_id\n",
    "\n",
    "    def decode_sentence(self, sent: list[int]) -> str:\n",
    "        return \" \".join([self.ids_to_word[id] for id in sent])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.words_to_id.__len__()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMLxenl58nYd"
   },
   "source": [
    "## Handy code to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZItD3XrL85ur"
   },
   "outputs": [],
   "source": [
    "def train_on_batch(model, x_batch, y_batch, optimizer, loss_function):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(x_batch.to(device))\n",
    "\n",
    "    # need to adjust output sizes to evaluate loss_function on batch\n",
    "    output  = output.reshape(-1, output.shape[2])\n",
    "    y_batch = y_batch.reshape(-1)\n",
    "\n",
    "    loss = loss_function(output, y_batch.to(model.device))\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    return loss.cpu().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MVNMl8DfmRhU"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(generator, model, loss_function, optimizer, callback = None):\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    for batch_of_x, batch_of_y in generator:\n",
    "\n",
    "        batch_loss = train_on_batch(model, batch_of_x, batch_of_y, optimizer, loss_function)\n",
    "\n",
    "        if callback is not None:\n",
    "            with torch.no_grad():\n",
    "                callback(model, batch_loss)\n",
    "\n",
    "        epoch_loss += batch_loss*len(batch_of_x)\n",
    "        total += len(batch_of_x)\n",
    "\n",
    "    return epoch_loss/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QCXwUfl51k3z"
   },
   "outputs": [],
   "source": [
    "def trainer(count_of_epoch,\n",
    "            batch_size,\n",
    "            dataset,\n",
    "            model,\n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            lr = 0.001,\n",
    "            callback = None):\n",
    "\n",
    "    optima = optimizer(model.parameters(), lr=lr)\n",
    "\n",
    "    iterations = tqdm(range(count_of_epoch), desc='epoch', leave=False)\n",
    "    iterations.set_postfix({'train epoch loss': np.nan})\n",
    "    for it in iterations:\n",
    "        batch_generator = tqdm(\n",
    "            torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True, pin_memory=True),\n",
    "            leave=False, total=len(dataset)//batch_size+(len(dataset)%batch_size>0))\n",
    "        epoch_loss = train_epoch (\n",
    "                    generator=batch_generator,\n",
    "                    model=model,\n",
    "                    loss_function=loss_function,\n",
    "                    optimizer=optima,\n",
    "                    callback=callback)\n",
    "\n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4RXQet33NIv",
    "outputId": "93ef2c10-44b0-4052-cdfd-7ec53c2e7233"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model_score(model, dataset, batch_size: int, loss_function, device = 'cpu'):\n",
    "    batch_generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "    pred = []\n",
    "    real = []\n",
    "    test_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for x_batch, y_batch in batch_generator:\n",
    "        output = model(x_batch.to(device))\n",
    "\n",
    "        output_loss  = output.reshape(-1, output.shape[2])\n",
    "        y_batch = y_batch.reshape(-1)\n",
    "\n",
    "        #evaluate loss on batch\n",
    "        batch_loss = loss_function(output_loss, y_batch.to(model.device))\n",
    "        test_loss += batch_loss.cpu().item() * len(x_batch)\n",
    "        total_samples += len(x_batch)\n",
    "\n",
    "        # add prediction results for classification report\n",
    "        output_report = torch.argmax(output, -1)\n",
    "        pred.extend(torch.reshape(output_report, (-1,)).cpu().numpy())\n",
    "        real.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    pred = np.array(pred)\n",
    "    real = np.array(real)\n",
    "\n",
    "    # need to mask and remove from real tags PAD, CLS, SEP, they are needed only to support LSTM architecture\n",
    "    mask = np.isin(real, [0, 1, 2], invert=True)\n",
    "    return classification_report(real[mask], pred[mask]), test_loss / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[UNK]': 3, 'NUM': 4, 'PUNCT': 5, 'SCONJ': 6, 'ADP': 7, 'VERB': 8, 'PROPN': 9, 'CCONJ': 10, 'ADJ': 11, 'ADV': 12, 'PRON': 13, 'NOUN': 14, 'DET': 15, 'PART': 16, 'AUX': 17, 'X': 18, 'SYM': 19, 'INTJ': 20}\n",
      "Tokenized: [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 11, 22, 23, 24, 25, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded: [CLS] Вице-премьер по социальным вопросам Татьяна Голикова рассказала , в каких регионах России зафиксирована наиболее высокая смертность от рака , сообщает РИА Новости . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Vocabulary total size 38665\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice as head\n",
    "from nerus import load_nerus\n",
    "\n",
    "MAX_SAMPLES = 1000\n",
    "\n",
    "dataset = head(load_nerus(\"dataset/nerus_lenta.conllu.gz\"), MAX_SAMPLES)\n",
    "nerus_tokenizer = NerusTokenizer()\n",
    "\n",
    "for sample in dataset:\n",
    "    nerus_tokenizer.expand_vocabulary(sample.sents)\n",
    "    nerus_tokenizer.expand_pos_tagging_vocab(sample.sents)\n",
    "\n",
    "print(nerus_tokenizer.tag_to_id)\n",
    "\n",
    "dataset_words_id = []\n",
    "dataset_tags_id = []\n",
    "dataset = head(load_nerus(\"dataset/nerus_lenta.conllu.gz\"), MAX_SAMPLES)\n",
    "for i, sample in enumerate(dataset):\n",
    "    words_id, tags = nerus_tokenizer.tokenize_sent_pos_tagging(sample.sents)\n",
    "    tags_id = nerus_tokenizer.tokenize_pos_tags(tags)\n",
    "\n",
    "    dataset_words_id.extend(words_id)\n",
    "    dataset_tags_id.extend(tags_id)\n",
    "\n",
    "print(f\"Tokenized: {dataset_words_id[0]}\")\n",
    "print(f\"Decoded: {nerus_tokenizer.decode_sentence(dataset_words_id[0])}\")\n",
    "print(f\"Vocabulary total size {len(nerus_tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Devide data train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11645, 50])\n",
      "torch.Size([11645, 50])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tags_tensor  = torch.tensor(dataset_tags_id)\n",
    "sents_tensor = torch.tensor(dataset_words_id)\n",
    "\n",
    "words_train, words_test = train_test_split(sents_tensor, test_size=0.2, random_state=52)\n",
    "tags_train, tags_test = train_test_split(tags_tensor, test_size=0.2, random_state=52)\n",
    "\n",
    "train_dataset_pt = torch.utils.data.TensorDataset(words_train, tags_train.long())\n",
    "test_dataset_pt = torch.utils.data.TensorDataset(words_test, tags_test.long())\n",
    "\n",
    "print(tags_tensor.shape)\n",
    "print(sents_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNclassifier(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    def __init__(self, output_dim, emb_dim=10, hidden_dim=10,\n",
    "                 num_layers=3, bidirectional=False, p=0.7, batchnorm=False):\n",
    "        super(RNNclassifier, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.biderect = bidirectional\n",
    "        self.dropout = p\n",
    "        self.batchnorm = batchnorm\n",
    "        self.vocab_size = len(nerus_tokenizer)\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(emb_dim, hidden_dim, num_layers,\n",
    "                                     bidirectional=bidirectional,\n",
    "                                     batch_first=True, dropout=p)\n",
    "\n",
    "        self.embending = torch.nn.Embedding(self.vocab_size, emb_dim, padding_idx=0)\n",
    "\n",
    "        self.norm = batchnorm\n",
    "        if batchnorm:\n",
    "            self.normalization = torch.nn.BatchNorm1d(MAX_SENTENCE_LENGTH)\n",
    "\n",
    "        self.linear = torch.nn.Linear(int(bidirectional + 1)*hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        encoded = self.embending(input)\n",
    "\n",
    "        lstm_out, _ = self.lstm(encoded.float())\n",
    "        if self.norm:\n",
    "            lstm_out = self.normalization(lstm_out)\n",
    "        return self.linear(lstm_out)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \\\n",
    "        f\"layer_size={self.hidden_dim}\" +     \\\n",
    "        f\"_layers_num={self.num_layers}\" +    \\\n",
    "        f\"_bidirect={int(self.biderect)}\" +   \\\n",
    "        f\"_dropout={self.dropout}\" +          \\\n",
    "        f\"_batchnorm={int(self.batchnorm)}\" + \\\n",
    "        f\"_vocab_size={self.vocab_size}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardCallback:\n",
    "    def __init__(self, writer, test_dataset, loss_function, delimeter=100, batch_size=64):\n",
    "        self.step = 0\n",
    "        self.writer = writer\n",
    "        self.delimeter = delimeter\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "    def forward(self, model, loss):\n",
    "        self.step += 1\n",
    "        self.writer.add_scalar(f'LOSS/train/{model}', loss, self.step)\n",
    "\n",
    "        if self.step % self.delimeter == 0:\n",
    "            self.writer.add_graph(model, torch.zeros(1, MAX_SENTENCE_LENGTH, dtype=torch.long).to(model.device))\n",
    "\n",
    "            model_metrics, model_loss = evaluate_model_score(model, self.test_dataset, self.batch_size, self.loss_function, model.device)\n",
    "\n",
    "            self.writer.add_scalar(f'LOSS/test/{model}', model_loss, self.step)\n",
    "            self.writer.add_text(f'METRICS/{model}', str(model_metrics), self.step)\n",
    "\n",
    "    def __call__(self, model, loss):\n",
    "        return self.forward(model, loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config['output_dim'] = nerus_tokenizer.tag_to_id.__len__()\n",
    "config['emb_dim'] = 128\n",
    "config['hidden_dim'] = 256\n",
    "config['num_layers'] = 2\n",
    "config['bidirectional'] = True\n",
    "config['batchnorm'] = False\n",
    "config['p'] = 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model learning dependency on layer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = []\n",
    "hidden_dims = [32, 64, 128, 256]\n",
    "dims_config = config.copy()\n",
    "\n",
    "\n",
    "for dim in hidden_dims:\n",
    "    dims_config['hidden_dim'] = dim\n",
    "    model = RNNclassifier(**dims_config)\n",
    "    model.to(device)\n",
    "    models_to_train.append(model)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=nerus_tokenizer.tag_to_id[\"[PAD]\"])\n",
    "optimizer = torch.optim.Adam\n",
    "\n",
    "for num_layers, model in zip(hidden_dims, models_to_train):\n",
    "    writer = SummaryWriter(log_dir=f'tensorboard/layer_width/{num_layers}')\n",
    "    nerus_callback = TensorboardCallback(writer, test_dataset_pt, loss_function)\n",
    "\n",
    "    trainer(count_of_epoch=10,\n",
    "        batch_size=64,\n",
    "        dataset=train_dataset_pt,\n",
    "        model=model,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        lr=0.001,\n",
    "        callback=nerus_callback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model learning dependency on number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = []\n",
    "num_layers = [2, 4, 8, 16]\n",
    "lay_config = config.copy()\n",
    "\n",
    "for num in num_layers:\n",
    "    lay_config['num_layers'] = num\n",
    "    model = RNNclassifier(**lay_config)\n",
    "    model.to(device)\n",
    "    models_to_train.append(model)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=nerus_tokenizer.tag_to_id[\"[PAD]\"])\n",
    "optimizer = torch.optim.Adam\n",
    "\n",
    "for num_layers, model in zip(num_layers, models_to_train):\n",
    "    writer = SummaryWriter(log_dir=f'tensorboard/num_layers/{num_layers}')\n",
    "    nerus_callback = TensorboardCallback(writer, test_dataset_pt, loss_function)\n",
    "\n",
    "    trainer(count_of_epoch=10,\n",
    "        batch_size=64,\n",
    "        dataset=train_dataset_pt,\n",
    "        model=model,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        lr=0.001,\n",
    "        callback=nerus_callback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model dependency on dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = []\n",
    "dropouts = [0.2, 0.5, 0.7, 0.9]\n",
    "p_config = config.copy()\n",
    "\n",
    "for d in dropouts:\n",
    "    p_config['p'] = d\n",
    "    model = RNNclassifier(**p_config)\n",
    "    model.to(device)\n",
    "    models_to_train.append(model)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=nerus_tokenizer.tag_to_id[\"[PAD]\"])\n",
    "optimizer = torch.optim.Adam\n",
    "\n",
    "for d, model in zip(dropouts, models_to_train):\n",
    "    writer = SummaryWriter(log_dir=f'tensorboard/dropout/{int(d * 10)}')\n",
    "    nerus_callback = TensorboardCallback(writer, test_dataset_pt, loss_function)\n",
    "\n",
    "    trainer(count_of_epoch=10,\n",
    "        batch_size=64,\n",
    "        dataset=train_dataset_pt,\n",
    "        model=model,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        lr=0.001,\n",
    "        callback=nerus_callback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model dependency on batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = []\n",
    "batch_norms = [False, True]\n",
    "batch_config = config.copy()\n",
    "\n",
    "for norm in batch_norms:\n",
    "    batch_config['batchnorm'] = norm\n",
    "    model = RNNclassifier(**batch_config)\n",
    "    model.to(device)\n",
    "    models_to_train.append(model)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=nerus_tokenizer.tag_to_id[\"[PAD]\"])\n",
    "optimizer = torch.optim.Adam\n",
    "\n",
    "for norm, model in zip(batch_norms, models_to_train):\n",
    "    writer = SummaryWriter(log_dir=f'tensorboard/batchnorm/{int(norm)}')\n",
    "    nerus_callback = TensorboardCallback(writer, test_dataset_pt, loss_function)\n",
    "\n",
    "    trainer(count_of_epoch=10,\n",
    "        batch_size=64,\n",
    "        dataset=train_dataset_pt,\n",
    "        model=model,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        lr=0.001,\n",
    "        callback=nerus_callback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model learning dependency on embending dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = []\n",
    "emb_dims = [10, 50, 100, 150, 256]\n",
    "emb_config = config.copy()\n",
    "\n",
    "for dim in emb_dims:\n",
    "    emb_config['emb_dim'] = dim\n",
    "    model = RNNclassifier(**emb_config)\n",
    "    model.to(device)\n",
    "    models_to_train.append(model)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=nerus_tokenizer.tag_to_id[\"[PAD]\"])\n",
    "optimizer = torch.optim.Adam\n",
    "\n",
    "for dim, model in zip(emb_dims, models_to_train):\n",
    "    writer = SummaryWriter(log_dir=f'tensorboard/embending_dim/{dim}')\n",
    "    nerus_callback = TensorboardCallback(writer, test_dataset_pt, loss_function)\n",
    "\n",
    "    trainer(count_of_epoch=10,\n",
    "        batch_size=64,\n",
    "        dataset=train_dataset_pt,\n",
    "        model=model,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        lr=0.001,\n",
    "        callback=nerus_callback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load report and investigate dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboardS'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreload_ext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtensorboardS\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mtensorboard\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m--logdir tensorboard/\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/machine-learning/.venv/lib64/python3.12/site-packages/IPython/core/interactiveshell.py:2481\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2479\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2480\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2481\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2483\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2484\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2485\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2486\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/machine-learning/.venv/lib64/python3.12/site-packages/IPython/core/magics/extension.py:63\u001b[39m, in \u001b[36mExtensionMagics.reload_ext\u001b[39m\u001b[34m(self, module_str)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_str:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UsageError(\u001b[33m'\u001b[39m\u001b[33mMissing module name.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextension_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreload_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/machine-learning/.venv/lib64/python3.12/site-packages/IPython/core/extensions.py:125\u001b[39m, in \u001b[36mExtensionManager.reload_extension\u001b[39m\u001b[34m(self, module_str)\u001b[39m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28mself\u001b[39m.loaded.add(module_str)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/machine-learning/.venv/lib64/python3.12/site-packages/IPython/core/extensions.py:62\u001b[39m, in \u001b[36mExtensionManager.load_extension\u001b[39m\u001b[34m(self, module_str)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load an IPython extension by its module name.\u001b[39;00m\n\u001b[32m     56\u001b[39m \n\u001b[32m     57\u001b[39m \u001b[33;03mReturns the string \"already loaded\" if the extension is already loaded,\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m\"no load function\" if the module doesn't have a load_ipython_extension\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03mfunction, or None if it succeeded.\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module_str \u001b[38;5;129;01min\u001b[39;00m BUILTINS_EXTS:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/machine-learning/.venv/lib64/python3.12/site-packages/IPython/core/extensions.py:77\u001b[39m, in \u001b[36mExtensionManager._load_extension\u001b[39m\u001b[34m(self, module_str)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.shell.builtin_trap:\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module_str \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys.modules:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m         mod = \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     mod = sys.modules[module_str]\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_load_ipython_extension(mod):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorboardS'"
     ]
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir tensorboard/\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
