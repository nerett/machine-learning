{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I.4 Variational autoencooder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import gridspec\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import requests\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU device!\")\n",
    "    device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianClustersDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_samples=10000, num_clusters=5, dim=10):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        np.random.seed(42)\n",
    "\n",
    "        for i in range(num_clusters):\n",
    "            mean = np.random.uniform(-5, 5, size=(dim,)) # cluster center\n",
    "            cov = np.eye(dim) * np.random.uniform(0.5, 1.5) # diag cov matrix\n",
    "\n",
    "            samples = np.random.multivariate_normal(mean, cov, num_samples // num_clusters)\n",
    "\n",
    "            self.data.append(samples)\n",
    "            self.labels.extend([i] * (num_samples // num_clusters))\n",
    "\n",
    "        self.data = np.vstack(self.data).astype(np.float32)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset = GaussianClustersDataset(num_samples=5000, num_clusters=5, dim=3)\n",
    "sample_data, sample_labels = example_dataset.data, example_dataset.labels\n",
    "\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "sc = ax.scatter(sample_data[:, 0], sample_data[:, 1], sample_data[:, 2], c=sample_labels, cmap='viridis', alpha=0.5)\n",
    "plt.colorbar(sc, label=\"Cluster ID\")\n",
    "\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")\n",
    "ax.set_zlabel(\"Feature 3\")\n",
    "plt.title(\"Gaussian Clusters dataset (3D Visualization)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_generator, model, loss_function, optimizer, scheduler=None, callback=None):\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "\n",
    "    for it, batch_of_x in enumerate(train_generator):\n",
    "        batch_loss = train_on_batch(model, batch_of_x[0], optimizer, loss_function)\n",
    "\n",
    "        if callback is not None:\n",
    "            with torch.no_grad():\n",
    "                callback(model, batch_loss)\n",
    "\n",
    "        epoch_loss += batch_loss * len(batch_of_x)\n",
    "        total += len(batch_of_x)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    return epoch_loss / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(count_of_epoch,\n",
    "            batch_size,\n",
    "            dataset,\n",
    "            model,\n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            lr=0.001,\n",
    "            callback=None):\n",
    "\n",
    "    optima = optimizer(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optima, gamma=0.95)\n",
    "\n",
    "    iterations = tqdm(range(count_of_epoch), desc='epoch')\n",
    "    iterations.set_postfix({'train epoch loss': np.nan})\n",
    "\n",
    "    for it in iterations:\n",
    "        batch_generator = tqdm(\n",
    "            torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True),\n",
    "            leave=False, total=len(dataset) // batch_size + (len(dataset) % batch_size > 0))\n",
    "\n",
    "        epoch_loss = train_epoch(train_generator=batch_generator,\n",
    "                    model=model,\n",
    "                    loss_function=loss_function,\n",
    "                    optimizer=optima,\n",
    "                    scheduler=scheduler,\n",
    "                    callback=callback)\n",
    "\n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, x_batch, optimizer, loss_function):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(x_batch.to(model.device))\n",
    "    loss = model.loss(*output)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    return loss.cpu().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback():\n",
    "    def __init__(self, writer, dataset, loss_function, experiment_name, delimeter=100, batch_size=64):\n",
    "        self.step = 0\n",
    "        self.writer = writer\n",
    "        self.delimeter = delimeter\n",
    "        self.loss_function = loss_function\n",
    "        self.experiment_name = experiment_name\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, model, loss):\n",
    "        self.step += 1\n",
    "        self.writer.add_scalar(f\"{self.experiment_name}/LOSS/train\", loss, self.step)\n",
    "\n",
    "        if self.step % self.delimeter == 0:\n",
    "\n",
    "            batch_generator = torch.utils.data.DataLoader(dataset = self.dataset,\n",
    "                                                          batch_size=self.batch_size)\n",
    "\n",
    "            pred = []\n",
    "            real = []\n",
    "            test_loss = 0\n",
    "            model.eval()\n",
    "            for it, (x_batch, _) in enumerate(batch_generator):\n",
    "                x_batch = x_batch.to(model.device)\n",
    "\n",
    "                output = model(x_batch)[0]\n",
    "\n",
    "                test_loss += self.loss_function(output, x_batch).cpu().item()*len(x_batch)\n",
    "\n",
    "                pred.extend(torch.argmax(output, dim=-1).cpu().numpy().tolist())\n",
    "\n",
    "            test_loss /= len(self.dataset)\n",
    "\n",
    "            self.writer.add_scalar(f\"{self.experiment_name}/LOSS/test\", test_loss, self.step)\n",
    "\n",
    "    def __call__(self, model, loss):\n",
    "        return self.forward(model, loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __init__(self, latent_dim, input_dim, hidden_dim=200, n_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        hidden_dims = list(np.linspace(input_dim, hidden_dim, n_layers, dtype=int))\n",
    "\n",
    "        self.fc_mu = torch.nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_var = torch.nn.Linear(hidden_dims[-1], latent_dim)\n",
    "\n",
    "        self.encoder_input = torch.nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.decoder_input = torch.nn.Linear(latent_dim, hidden_dims[-1])\n",
    "\n",
    "        encoder_modules = []\n",
    "        decoder_modules = []\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            encoder_modules.append(\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.Linear(hidden_dims[i], hidden_dims[i + 1]),\n",
    "                    torch.nn.BatchNorm1d(hidden_dims[i + 1]),\n",
    "                    torch.nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            decoder_modules.append(\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.Linear(hidden_dims[i], hidden_dims[i + 1]),\n",
    "                    torch.nn.BatchNorm1d(hidden_dims[i + 1]),\n",
    "                    torch.nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(self.encoder_input, *encoder_modules)\n",
    "        self.decoder = torch.nn.Sequential(self.decoder_input, *decoder_modules)\n",
    "\n",
    "        self.final_layer = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(hidden_dims[-1], hidden_dims[-1]),\n",
    "                            torch.nn.BatchNorm1d(hidden_dims[-1]),\n",
    "                            torch.nn.LeakyReLU(),\n",
    "                            torch.nn.Linear(hidden_dims[-1], input_dim))\n",
    "\n",
    "    def encode(self, input):\n",
    "        \"\"\"\n",
    "        Generates distribution provided by input.\n",
    "        Args:\n",
    "            input: Tensor - the matrix of shape batch_size x input_dim.\n",
    "        Returns:\n",
    "            List[mu, log_var] - the normal distribution parameters.\n",
    "            mu: Tensor - the matrix of shape batch_size x latent_dim.\n",
    "            sigma: Tensor - the matrix of shape batch_size x latent_dim.\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Maps the given latent codes onto the image space.\n",
    "        Args:\n",
    "            z: Tensor - the matrix of shape batch_size x latent_dim.\n",
    "        Returns:\n",
    "            Tensor - decoded sample.\n",
    "        \"\"\"\n",
    "        result = self.decoder(z)\n",
    "        result = self.final_layer(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def sample_z(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Generates sample from normal distribution N(mu, var).\n",
    "        Args:\n",
    "            mu: Tensor - the matrix of shape batch_size x latent_dim.\n",
    "            sigma: Tensor - the matrix of shape batch_size x latent_dim.\n",
    "        Returns:\n",
    "            Tensor - the tensor of shape batch_size x latent_dim - samples from normal distribution in latent space.\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.sample_z(mu, log_var)\n",
    "\n",
    "        return [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        recons_loss = torch.nn.functional.mse_loss(recons, input)\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0) # KL-divergention\n",
    "\n",
    "        loss = recons_loss + kld_loss\n",
    "        return loss\n",
    "\n",
    "    def generate_samples(self, num_samples:int, **kwargs):\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding image space map.\n",
    "        Args:\n",
    "            num_samples: int - the number of samples, witch need to generate.\n",
    "        Returns:\n",
    "            Tensor - the matrix of shape num_samples x input_dim.\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples, self.latent_dim)\n",
    "        z = z.to(self.device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        Generate decoded sample after encoding.\n",
    "        Args:\n",
    "            x: Tensor - the matrix of shape batch_size x input_dim.\n",
    "        Returns:\n",
    "            Tensor - decoded sample after encoding of x.\n",
    "        \"\"\"\n",
    "        return self.forward(x)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ParameterGrid({\n",
    "    'hidden_dim': [64, 512, 1024],\n",
    "    'input_dim': [2, 64, 256],\n",
    "    'num_layers': [2, 16, 64],\n",
    "    'latent_dim': [2, 128, ],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set in grid:\n",
    "    synthetic_dataset = GaussianClustersDataset(num_samples=40000, num_clusters=5, dim=set['input_dim'])\n",
    "    synthetic_train, synthetic_test = torch.utils.data.random_split(synthetic_dataset, [0.8, 0.2])\n",
    "\n",
    "    autoencoder = VAE(latent_dim=set['latent_dim'], input_dim=set['input_dim'], hidden_dim=set['hidden_dim'])\n",
    "    autoencoder.to(device)\n",
    "\n",
    "    writer = SummaryWriter(log_dir = f\"tensorboard/in={set['input_dim']}_hi={set['hidden_dim']}_lat={set['latent_dim']}_lay={set['num_layers']}\")\n",
    "    call = callback(writer, synthetic_test, torch.nn.MSELoss(), experiment_name=\"Overall\", delimeter=100)\n",
    "\n",
    "    trainer(count_of_epoch=10,\n",
    "            batch_size=256,\n",
    "            dataset=synthetic_train,\n",
    "            model=autoencoder,\n",
    "            loss_function=autoencoder.loss,\n",
    "            optimizer = optimizer,\n",
    "            lr = 0.001,\n",
    "            callback = call)\n",
    "\n",
    "#     if set['input_dim'] == 2:\n",
    "#             distr = autoencoder.generate_samples(1000)\n",
    "#             x, y = np.array(distr.detach().cpu()).T\n",
    "\n",
    "#             plt.figure(figsize=(5, 4), dpi=100)\n",
    "#             decoding = autoencoder.generate(distr).detach().cpu()\n",
    "\n",
    "#             plt.scatter(x, y, label='Input')\n",
    "#             plt.scatter(decoding[:, 0], decoding[:, 1], label='Generated')\n",
    "#             plt.axis('equal')\n",
    "#             plt.legend()\n",
    "#             writer.add_figure('Visual', plt.gcf())\n",
    "#             plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./tensorboard/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
