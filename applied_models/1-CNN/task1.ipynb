{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./task1.png)\n",
    "\n",
    "- [src-1](https://habr.com/ru/companies/skillfactory/articles/565232/)\n",
    "- [src-2](https://ru.wikipedia.org/wiki/%D0%A1%D0%B2%D1%91%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "from copy import deepcopy\n",
    "from lime import lime_image\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.image import imread\n",
    "from mpl_toolkits import mplot3d\n",
    "from PIL import Image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from urllib.request import urlopen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, x_batch, y_batch, optimizer, loss_function):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    output = model(x_batch.to(device))\n",
    "\n",
    "    loss = loss_function(output, y_batch.to(device))\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    return loss.cpu().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_generator, model, loss_function, optimizer, callback = None):\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
    "        batch_loss = train_on_batch(model, batch_of_x.to(device), batch_of_y.to(device), optimizer, loss_function)\n",
    "\n",
    "        if callback is not None:\n",
    "            callback(model, batch_loss)\n",
    "\n",
    "        epoch_loss += batch_loss*len(batch_of_x)\n",
    "        total += len(batch_of_x)\n",
    "\n",
    "    return epoch_loss/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(count_of_epoch,\n",
    "            batch_size,\n",
    "            dataset,\n",
    "            model,\n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            lr = 0.001,\n",
    "            callback = None):\n",
    "\n",
    "    optima = optimizer(model.parameters(), lr=lr)\n",
    "\n",
    "    iterations = tqdm(range(count_of_epoch), desc='epoch')\n",
    "    iterations.set_postfix({'train epoch loss': np.nan})\n",
    "    for it in iterations:\n",
    "        batch_generator = tqdm(\n",
    "            torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True),\n",
    "            leave=False, total=len(dataset)//batch_size+(len(dataset)%batch_size> 0))\n",
    "\n",
    "        epoch_loss = train_epoch(train_generator=batch_generator,\n",
    "                    model=model,\n",
    "                    loss_function=loss_function,\n",
    "                    optimizer=optima,\n",
    "                    callback=callback)\n",
    "\n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parametrizable agile class to speed up investigations\n",
    "class CNNParametrizable(torch.nn.Module):\n",
    "    __DEFAULT_IS_POOLING__ : bool = True\n",
    "    __DEFAULT_BATCHNORM__  : bool = False\n",
    "\n",
    "    __DEFAULT_DROPOUT__ : float = -1.0\n",
    "\n",
    "    __DEFAULT_CONV_LAYERS__   : int = 2\n",
    "    __DEFAULT_LINEAR_LAYERS__ : int = 3\n",
    "\n",
    "    __DEFAULT_CONV_SIZE__    : list[int] = [5]*__DEFAULT_CONV_LAYERS__\n",
    "    __DEFAULT_POOLING_SIZE__ : list[int] = [2]*__DEFAULT_CONV_LAYERS__\n",
    "    __DEFAULT_CONV_CHANNELS_ : list[int] = [1, 6, 16]\n",
    "\n",
    "    conv_sz:    list[int] = __DEFAULT_CONV_SIZE__\n",
    "    conv_chnls: list[int] = __DEFAULT_CONV_CHANNELS_\n",
    "\n",
    "    is_pooling:   bool = __DEFAULT_IS_POOLING__\n",
    "    pool_sz: list[int] = __DEFAULT_POOLING_SIZE__\n",
    "\n",
    "    add_batchnorm : bool = __DEFAULT_BATCHNORM__\n",
    "    dropout : float = __DEFAULT_DROPOUT__\n",
    "\n",
    "    # number of each type layers\n",
    "    num_conv: int = __DEFAULT_CONV_LAYERS__\n",
    "    num_lin:  int = __DEFAULT_LINEAR_LAYERS__\n",
    "\n",
    "    # creates layers based on provided params\n",
    "    class SequentialBuilder:\n",
    "        model: torch.nn.Module\n",
    "\n",
    "        # stores output size after convolution\n",
    "        output_conv_sz: int\n",
    "        # stores output size from previous linear layer\n",
    "        prev_linear_sz: int\n",
    "\n",
    "        curr_conv_i: int\n",
    "        curr_lin_i: int\n",
    "\n",
    "        # input MNIST x*y sizes\n",
    "        SIZE_X: int = 28\n",
    "        SIZE_Y: int = 28\n",
    "\n",
    "        def __init__(self, model: torch.nn.Module):\n",
    "            self.model = model\n",
    "\n",
    "            self.output_conv_sz, self.prev_linear_sz = (self.__calc_output_conv(),) * 2\n",
    "\n",
    "            self.curr_conv_i = 0\n",
    "            self.curr_lin_i = 0\n",
    "\n",
    "        def add_layer_conv(self):\n",
    "            input_channels  = self.model.conv_chnls[self.curr_conv_i]\n",
    "            output_channels = self.model.conv_chnls[self.curr_conv_i + 1]\n",
    "            kernel_sz = self.model.conv_sz[self.curr_conv_i]\n",
    "\n",
    "            self.model.layers.add_module(f\"conv{self.curr_conv_i}\", torch.nn.Conv2d(input_channels, output_channels, kernel_sz))\n",
    "            if self.model.add_batchnorm:\n",
    "                self.model.layers.add_module(f\"bnorm{self.curr_conv_i}\", torch.nn.BatchNorm2d(output_channels))\n",
    "            self.model.layers.add_module(f\"conv-relu{self.curr_conv_i}\", torch.nn.ReLU())\n",
    "\n",
    "            # add polling\n",
    "            if (self.model.is_pooling):\n",
    "                kernel_pool_sz = self.model.pool_sz[self.curr_conv_i]\n",
    "                self.model.layers.add_module(f\"pool{self.curr_conv_i}\", torch.nn.MaxPool2d(kernel_pool_sz))\n",
    "\n",
    "            self.curr_conv_i += 1\n",
    "\n",
    "        def add_flatten(self):\n",
    "            self.model.layers.add_module(\"flatten\", torch.nn.Flatten())\n",
    "\n",
    "        def add_layer_lin(self):\n",
    "            def output_linear_proportional(num_lin) -> int:\n",
    "                return int((1 - self.curr_lin_i / num_lin) * 0.8 * self.output_conv_sz)\n",
    "            # we are adding last linear layer\n",
    "            if self.curr_lin_i == self.model.num_lin - 1:\n",
    "                output_linear_sz = 10\n",
    "            else:\n",
    "                output_linear_sz = max(output_linear_proportional(self.model.num_lin), 10)\n",
    "\n",
    "            self.model.layers.add_module(f\"linear{self.curr_lin_i}\", torch.nn.Linear(self.prev_linear_sz, output_linear_sz))\n",
    "            if (self.curr_lin_i != self.model.num_lin - 1):\n",
    "                self.add_dropout(self.curr_lin_i)\n",
    "                self.model.layers.add_module(f\"lin-relu{self.curr_lin_i}\", torch.nn.ReLU())\n",
    "\n",
    "            self.prev_linear_sz = output_linear_sz\n",
    "            self.curr_lin_i += 1\n",
    "\n",
    "        def add_dropout(self, idx):\n",
    "            if self.model.dropout > 0:\n",
    "                self.model.layers.add_module(f\"dropout{idx}\", torch.nn.Dropout(self.model.dropout))\n",
    "\n",
    "        def __calc_output_conv(self) -> int:\n",
    "            spatialX = self.SIZE_X\n",
    "            spatialY = self.SIZE_Y\n",
    "\n",
    "            if self.model.is_pooling:\n",
    "                for conv_sz, pool_sz in zip(self.model.conv_sz, self.model.pool_sz):\n",
    "                    spatialX -= (conv_sz - 1)\n",
    "                    spatialY -= (conv_sz - 1)\n",
    "                    spatialX //= pool_sz\n",
    "                    spatialY //= pool_sz\n",
    "            else:\n",
    "                for conv_sz in self.model.conv_sz:\n",
    "                    spatialX -= (conv_sz - 1)\n",
    "                    spatialY -= (conv_sz - 1)\n",
    "\n",
    "            return int(spatialX * spatialY * self.model.conv_chnls[-1])\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        for p in self.parameters():\n",
    "            return p.device\n",
    "\n",
    "    def __init__(self, **params):\n",
    "        super(CNNParametrizable, self).__init__()\n",
    "        # check all params presented are valid\n",
    "        for name in params.keys():\n",
    "            self.__getattribute__(name)\n",
    "\n",
    "        # setting values on init\n",
    "        for name, value in params.items():\n",
    "            if value is not None:\n",
    "                self.__setattr__(name, value)\n",
    "\n",
    "        is_comp, msg = self.is_compatible()\n",
    "        if not is_comp:\n",
    "            raise RuntimeError(msg)\n",
    "\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        builder = self.SequentialBuilder(self)\n",
    "        # build parametrized CNN\n",
    "        for _ in range(self.num_conv):\n",
    "            builder.add_layer_conv()\n",
    "        builder.add_flatten()\n",
    "        for _ in range(self.num_lin):\n",
    "            builder.add_layer_lin()\n",
    "\n",
    "    # checks if entered parametrs are compatible with each other\n",
    "    def is_compatible(self) -> tuple[bool, str]:\n",
    "        if self.num_conv <= 0:\n",
    "            return False, \"num_conv <= 0\"\n",
    "        if self.num_lin <= 0:\n",
    "            return False, \"num_lin <= 0\"\n",
    "        if len(self.conv_sz) != self.num_conv:\n",
    "            return False, f\"len(conv_sz) == {len(self.conv_sz)} != {self.num_conv} == num of conv layers\"\n",
    "        if len(self.conv_chnls) - 1 != self.num_conv:\n",
    "            return False, f\"len(conv_chnls) == {len(self.conv_chnls)} != {self.num_conv} == num of conv layers\"\n",
    "        if self.conv_chnls[0] != 1:\n",
    "            return False, \"first input channel must be 1 sized\"\n",
    "        if self.is_pooling and len(self.pool_sz) != self.num_conv:\n",
    "            return False, f\"len(pool_sz) == {len(self.pool_sz)} != {self.num_conv} == num of conv layers\"\n",
    "        if self.dropout > 1:\n",
    "            return False, f\"dropout = {self.dropout} > 1\"\n",
    "\n",
    "        return True, \"Compatable!\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNParametrizable(\n",
       "  (layers): Sequential(\n",
       "    (conv0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (bnorm0): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv-relu0): ReLU()\n",
       "    (pool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv1): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (bnorm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv-relu1): ReLU()\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(16, 20, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bnorm2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv-relu2): ReLU()\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear0): Linear(in_features=20, out_features=16, bias=True)\n",
       "    (dropout0): Dropout(p=0.3, inplace=False)\n",
       "    (lin-relu0): ReLU()\n",
       "    (linear1): Linear(in_features=16, out_features=12, bias=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (lin-relu1): ReLU()\n",
       "    (linear2): Linear(in_features=12, out_features=10, bias=True)\n",
       "    (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    (lin-relu2): ReLU()\n",
       "    (linear3): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = CNNParametrizable(\n",
    "    num_lin=4,\n",
    "    is_pooling=True,\n",
    "    pool_sz=[2, 2, 2],\n",
    "    num_conv=3,\n",
    "    conv_chnls=[1,6,16,20],\n",
    "    conv_sz=[5, 5, 3],\n",
    "    add_batchnorm=True,\n",
    "    dropout=0.3)\n",
    "my_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_train = datasets.MNIST('./mnist', train=True, download=True, transform=transforms.ToTensor())\n",
    "MNIST_test = datasets.MNIST('./mnist', train=False, download=True, transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback():\n",
    "    def __init__(self, writer, dataset, loss_function, delimeter = 100, batch_size=64):\n",
    "        self.step = 0\n",
    "        self.writer = writer\n",
    "        self.delimeter = delimeter\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, model, loss):\n",
    "        self.step += 1\n",
    "        self.writer.add_scalar('LOSS/train', loss, self.step)\n",
    "\n",
    "        if self.step % self.delimeter == 0:\n",
    "\n",
    "            self.writer.add_graph(model, self.dataset[0][0].view(1,1,28,28).to(model.device))\n",
    "\n",
    "            batch_generator = torch.utils.data.DataLoader(dataset = self.dataset,\n",
    "                                                          batch_size=self.batch_size)\n",
    "\n",
    "            pred = []\n",
    "            real = []\n",
    "            test_loss = 0\n",
    "            for it, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "                x_batch = x_batch.to(model.device)\n",
    "                y_batch = y_batch.to(model.device)\n",
    "\n",
    "                output = model(x_batch)\n",
    "\n",
    "                test_loss += self.loss_function(output, y_batch).cpu().item()*len(x_batch)\n",
    "\n",
    "                pred.extend(torch.argmax(output, dim=-1).cpu().numpy().tolist())\n",
    "                real.extend(y_batch.cpu().numpy().tolist())\n",
    "\n",
    "            test_loss /= len(self.dataset)\n",
    "\n",
    "            self.writer.add_scalar('LOSS/test', test_loss, self.step)\n",
    "\n",
    "            self.writer.add_text('REPORT/test', str(classification_report(real, pred)), self.step)\n",
    "    def __call__(self, model, loss):\n",
    "        return self.forward(model, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='tensorboard')\n",
    "call = callback(writer, MNIST_test, loss_function, delimeter=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e988e00ef81741b5bf5ded613331a90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer(count_of_epoch=5,\n",
    "        batch_size=64,\n",
    "        dataset=MNIST_train,\n",
    "        model=my_model,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        lr=0.001,\n",
    "        callback=call)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 10447), started 0:00:06 ago. (Use '!kill 10447' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4195789554a6083c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4195789554a6083c\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir tensorboard/ --port 6009\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6460663062095642\n"
     ]
    }
   ],
   "source": [
    "batch_generator = torch.utils.data.DataLoader(dataset = MNIST_test,\n",
    "                                              batch_size=64)\n",
    "\n",
    "pred = []\n",
    "real = []\n",
    "test_loss = 0\n",
    "for it, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "    x_batch = x_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "\n",
    "    output = my_model(x_batch)\n",
    "\n",
    "    test_loss += loss_function(output, y_batch).cpu().item()*len(x_batch)\n",
    "\n",
    "    pred.extend(torch.argmax(output, dim=-1).cpu().numpy().tolist())\n",
    "    real.extend(y_batch.cpu().numpy().tolist())\n",
    "\n",
    "test_loss /= len(MNIST_test)\n",
    "\n",
    "print('loss: {}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
